머신러닝이 하는 일
입력과 출력 간의 관계를 찾는 일
입력을 출력으로 변환하는 올바른 알고리즘을 찾는다

전통적인 개발
입력과 알고리즘을 알고 출력을 생성한느 함수를 작성

머신러닝
입력과 출력을 알고있지만
입력이 주어졌을 때 
출력을 생성하는 알고리즘은 알려지지 않음
-> 알고리즘을 학습한다

Feature : input을 의미
Lables : output을 의미
Examples : 모델에 한 쌍의 input과 output을 제공
	이 한 쌍을 의미

모델 생성
Layer 생성
l0(layer 0)
=  tf.keras.layers.Dense(units, input_shape=[])

input_shape[1] : 이 레이어에 single value를 투입한다
-> 1차원 배열
이 레이어가 첫번째이기 때문에 이 input shape가 
전체 모델의 input shape가 된다

units = 1 : 레이어의 뉴런 수를 결정
뉴런의 수는 레이어가 문제를 해결하는 방법을 배우기
위해 얼마나 많은 내부 변수를 가져야 하는지 정의
이 레이어가 마지막 레이어이기 때문에 모델의 
출력 사이즈와 같다

multi-layered network의 경우 
레이어의size와 shape는 다음 레이어의 input_shape와
맞춰주어야 한다

레이어를 모델에 합치기
model = tf.keras.Sequential([l0, l1, l2])
레이어를 생성했으면 이를 모델에 합쳐야 한다
순차적 모델 정의는 
input에서 output까지의 계산순서를 지정하는
매개변수로 레이어 리스트를 가진다.


컴파일
model.complie(loss, optimizer)
loss = 'mean_squared_error'
 : 원하는 결과로부터 예측이 얼마나 멀리 떨어져있는지
측정. 
측정된 차이 = 손실
loss가 크다는 것 = 옳은 값과 거리가 멀다
optimizer = tf.keras.optimizers.Adam(0.1)
 : loss를 줄이기 위해 내부 값을 조정
모델의 내부 변수 조정을 계산하는데 사용
*0.1 : 학습률(조정 가능)
(섭씨 화씨 변환 머신러닝에서 
0.1~0.001사이의 값은 의미있을 수 있다)
값이 너무 작으면 모형 교육에 너무 많은 반복을 수행
값이 너무 크면 정확도가 떨어진다
일반적으로 0.001(기본값), 0.1 이내

모델 훈련
 : 모델의 현재 손실을 계산하고 그것을 개선하는 행위
내부 변수를 최선의 값으로 조정해 입력을 출력에
매핑(하나의 값을 다른 값으로 대응)할 수 있도록 하는 것
(by Gradient Descent)

목표 : 손실을 최소화하는 것

fit method에 의해 제어된다
model.fit(입력, 출력, epochs, verbose=True/False)
epochs : Examples의 반복
epochs = 1 : 1회 반복
verbose : method가 생성하는 출력량 제어

처음에 내부변수(가중치)는 랜덤으로 설정되기 때문에
출력값이 올바른 값에 가깝지 않다.
실제 출력과 원하는 출력의 차이를 loss function을
이용해 계산하고, optimizer function은 가중치를
조정하는 방법을 지시한다

손실이 측정된 이후 모든 레이어의 내부 변수의 
neural network는 loss를 최소화하는 방향으로 조절된다
-> 결과값을 정답에 가깝게 만든다
Optimization process = Gradient Descent

더 많은 epochs를 반복할수록 손실률이 떨어진다

해당 과정에 대해 더 알고 싶은 경우 참조 :
https://developers.google.com/machine-learning/crash-course/reducing-loss/video-lecture


matplotlib -> 데이터 시각화


model.predict([input])

용어
Layer : neural network안의 서로 연결된 노드의 모임
Model : neural network를 대표하는 것
Dense and Fully Connected(FC) 
: 각 노드(in one layer)가 이전 계층의 각 노드와 연결되는 것
weights and biases : 내부 변수
loss : 원하는 값과 실제 값의 차이(discrepancy)
MSE : Mean squared error ★?
 Mean squared error, a type of loss function 
that counts a small number of large discrepancies 
as worse than a large number of small ones.

Gradient Descent : 내부 변수를 조금씩 움직이면서 
loss function을 줄이는 알고리즘
Optimizer : Gradient Descent의 특정 구현
ADAM = ADAptive with Momentum
Learning rate : Gradient Descent 안에서 손실 개선을 위한
단계 크기
Epoch : 전체 트레이닝 세트에 대한 전체 흐름
Forward pass : output 값 계산 form input
Backward pass(backpropagation)
 : 내부변수 조절 계산
(출력 레이어에서 시작하여 각 레이어를 통해 입력까지
다시 작업하는 optimizer algorithm에 의한)


Dense Layer
레이어 구성 > neuran
각 레이어의 뉴런은 다음 레이어의 뉴런과 연결될 수 있다
input layer -연결-> hidden layer -연결-> output layer
 : 해당 계층의 뉴런이 이전 계층의 뉴런과 완전히 연결되었다

input값 * weight + biase

실제 머신러닝
알고리즘와 변수를 일치시킬 수 없다
-> 목표 알고리즘조차 알 수 없기 떄문
알고리즘을 알았다면 전통적인 방법으로 진행했을 것

일반적으로 시행착오를 통해 레이어와 뉴런수를
바꿔가면서 시도하고 모델이 훈련단계에서
문제를 해결할 수 있는지 확인한다